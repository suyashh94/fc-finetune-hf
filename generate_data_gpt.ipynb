{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_training_data import functions, questions_dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'adjust_temperature',\n",
       " 'description': 'Adjust the temperature in a specified zone of the car.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'zone': {'type': 'string',\n",
       "    'enum': ['front', 'rear', 'all'],\n",
       "    'description': 'The zone where the temperature will be adjusted.',\n",
       "    'default': 'all'},\n",
       "   'temperature': {'type': 'number',\n",
       "    'description': 'The target temperature for the specified zone in degrees Celsius.'}},\n",
       "  'required': ['temperature'],\n",
       "  'optional': ['zone']}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ml-gpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import AzureChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultSchema(BaseModel):\n",
    "    # function_called_with_params: str = Field(..., description=\"Exact function with parameters to be called\")\n",
    "    user_query: str = Field(..., description=\"User query that caused the function to be called\")\n",
    "    \n",
    "    def model_json_schema():\n",
    "        return {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                # \"function_called_with_params\": {\"type\": \"string\"},\n",
    "                \"user_query\": {\"type\": \"string\"},\n",
    "            },\n",
    "            \"required\": [\"function_called\", \"user_query\"],\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTQueryGenerator:\n",
    "    def __init__(self):\n",
    "        self.llm = AzureChatOpenAI(\n",
    "                azure_endpoint = os.getenv(\"AZURE_ENDPOINT\"),\n",
    "                openai_api_key = os.getenv(\"OPENAI_API_KEY\"),\n",
    "                deployment_name=os.getenv(\"DEPLOYMENT_NAME\"),  # or your deployment\n",
    "                openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),  # or your api version\n",
    "                temperature=0.9,\n",
    "                max_tokens=None,\n",
    "                timeout=None,\n",
    "                max_retries=2,\n",
    "            )\n",
    "\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ResultSchema)\n",
    "        \n",
    "        self.prompt = PromptTemplate(template = '''\n",
    "                                     You have been provided with a complete function description.A function has multiple parameters and each \n",
    "                                     parameter type and possible values for that parameter, have been provided too, if there is a limit in choice. \\nThere are some \n",
    "                                     required and some optional parameters.\n",
    "                                     \\nYou have to come up with a user command that will call this function.\\nAssume you are someone driving the car when generating the command.\\n \n",
    "                                     The user command need not have optional parameters in it.\\n\n",
    "                                     Function Description: {function_description}\\n\n",
    "                                     \\n{format_instructions}\\n\n",
    "                                      \n",
    "                                     ''', \n",
    "                                     input_variables=[\"function_description\"],\n",
    "                                     partial_variables={\"format_instructions\": self.parser.get_format_instructions()}\n",
    "                                     )\n",
    "    \n",
    "    def get_result(self, function_description: str) -> ResultSchema:\n",
    "        chain = self.prompt | self.llm | self.parser\n",
    "        response = chain.invoke({\"function_description\": function_description})\n",
    "        print(response)\n",
    "        # try:\n",
    "        #     response = chain.invoke({\"function_description\": function_description})\n",
    "        #     print(response)\n",
    "        \n",
    "        # except Exception as e:\n",
    "        #     return ResultSchema(function_called=\"None\", user_query=\"None\")\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_query='Set the temperature to 22 degrees.'\n",
      "user_query='Set the temperature to 22 degrees Celsius in the front zone.'\n",
      "user_query='Set the temperature to 22 degrees.'\n",
      "user_query='Adjust the temperature to 22 degrees Celsius.'\n",
      "user_query='Set the temperature to 22 degrees in the front zone.'\n",
      "user_query='Set the temperature to 22 degrees Celsius.'\n",
      "user_query='Adjust the temperature to 22 degrees Celsius in the front zone.'\n",
      "user_query='Set the temperature to 22 degrees in the front zone.'\n",
      "user_query='Set the temperature to 22 degrees Celsius'\n"
     ]
    }
   ],
   "source": [
    "query_generator = GPTQueryGenerator()\n",
    "for i in range(10):\n",
    "    try:\n",
    "        query_generator.get_result(functions[0])\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
